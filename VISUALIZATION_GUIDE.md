# Visualization Guide

## Overview

The `visualize_results.py` script creates comprehensive visualizations from analysis results generated by `analyze_results.py`.

## Usage

### Basic Usage

```bash
# Create all visualizations from analysis JSON
python visualize_results.py --analysis-file analysis_report.json --output-dir visualizations

# Create specific visualization
python visualize_results.py --analysis-file analysis_report.json --plot summary

# Use with different model results
python visualize_results.py --analysis-file analysis_7B.json --output-dir visualizations_7B
python visualize_results.py --analysis-file analysis_2B.json --output-dir visualizations_2B
```

### Arguments

- `--analysis-file` (required): Path to analysis JSON file from `analyze_results.py`
- `--output-dir`: Directory to save plots (default: same directory as analysis file)
- `--plot`: Specific plot to create:
  - `all` (default): Create all visualizations
  - `summary`: Overall accuracy summary
  - `question_type`: Accuracy by question type
  - `answer_length`: Answer length analysis
  - `vocabulary`: Vocabulary error analysis
  - `special`: Special question types (spatial, numerical, color)
  - `prediction_length`: Prediction length analysis
  - `semantic`: Semantic similarity distribution
  - `answer_type`: Answer type analysis

## Generated Visualizations

### 1. Overall Summary (`01_overall_summary.png`)
- Bar chart showing accuracy for each evaluation method
- Color-coded by performance level
- Helps understand which metrics show best performance

### 2. Question Type Accuracy (`02_question_type_accuracy.png`)
- Horizontal bar chart of accuracy by question type
- Color-coded: Red (<30%), Orange (30-50%), Green (>50%)
- Shows which question types are most challenging

### 3. Answer Length Analysis (`03_answer_length_analysis.png`)
- Two-panel plot:
  - Left: Accuracy by answer length (1-10 words)
  - Right: Distribution of answer lengths in dataset
- Reveals if shorter/longer answers are easier

### 4. Vocabulary Analysis (`04_vocabulary_analysis.png`)
- Two-panel plot showing top error-prone keywords:
  - Left: Keywords in questions
  - Right: Keywords in situation descriptions
- Identifies problematic terms/concepts

### 5. Special Question Types (`05_special_question_types.png`)
- Bar chart comparing:
  - Spatial questions (left, right, direction)
  - Numerical questions (counting, numbers)
  - Color questions
- Shows performance on specific reasoning types

### 6. Prediction Length Analysis (`06_prediction_length_analysis.png`)
- Bar chart showing accuracy when:
  - Predictions are much longer than GT
  - Predictions are much shorter than GT
  - Predictions are similar length to GT
- Reveals if over/under-explanation affects accuracy

### 7. Semantic Similarity (`07_semantic_similarity.png`)
- Bar chart comparing semantic similarity scores:
  - Errors: Mean and range
  - Correct: Mean and range
- Shows how semantically different errors are from correct answers

### 8. Answer Type Analysis (`08_answer_type_analysis.png`)
- Horizontal bar chart of accuracy by answer type
- Based on dataset metadata
- Shows if certain answer categories are harder

## Complete Workflow

```bash
# Step 1: Run evaluation
python evaluate_sqa.py --split test --max-samples 100 --output results.json

# Step 2: Analyze results
python analyze_results.py --results-file results.json --output analysis.json

# Step 3: Create visualizations
python visualize_results.py --analysis-file analysis.json --output-dir visualizations
```

## Comparing Models

To compare different models visually:

```bash
# Analyze and visualize 7B model
python analyze_results.py --results-file sqa_test_results_7B.json --output analysis_7B.json
python visualize_results.py --analysis-file analysis_7B.json --output-dir visualizations_7B

# Analyze and visualize 2B model
python analyze_results.py --results-file sqa_test_results_2B.json --output analysis_2B.json
python visualize_results.py --analysis-file analysis_2B.json --output-dir visualizations_2B

# Compare the PNG files side by side
```

## Tips

1. **High-resolution output**: All plots are saved at 300 DPI for publication quality
2. **Color coding**: Consistent color scheme across plots (red=low, orange=medium, green=high)
3. **Value labels**: All bars include accuracy values and sample sizes
4. **Layout**: Plots are optimized for readability with proper spacing and labels

## Customization

To customize visualizations, edit `visualize_results.py`:
- Change colors: Modify color values in plot methods
- Adjust figure sizes: Change `figsize` parameters
- Modify styles: Update `sns.set_style()` or `plt.rcParams`
- Add new plots: Create new methods following the existing pattern

## Output Format

- **Format**: PNG images
- **Resolution**: 300 DPI
- **Naming**: Sequential numbering for easy ordering
- **Location**: Specified output directory (default: same as analysis file)

## Example Insights from Visualizations

1. **Question Type**: If "where" questions have low accuracy, focus on spatial reasoning
2. **Answer Length**: If 1-word answers have high accuracy but 3+ words are low, model struggles with complex answers
3. **Vocabulary**: If "dryer" appears in top error keywords, model may need more training on that concept
4. **Prediction Length**: If "pred_longer" has lower accuracy, model tends to over-explain incorrectly
5. **Semantic Similarity**: If errors have very low similarity scores, model is completely off-track

## Requirements

- matplotlib >= 3.7.0
- seaborn >= 0.12.0
- numpy (for calculations)

Install with:
```bash
pip install matplotlib seaborn
```

